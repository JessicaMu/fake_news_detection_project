{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d93fba-2cfb-427e-b55c-7456fd376669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Open and read the fake news CSV files\n",
    "fake_file_path = 'articles\\\\fake_news.csv'\n",
    "real_file_path = 'articles\\\\real_news.csv'\n",
    "\n",
    "fake_label = 0\n",
    "real_label = 1\n",
    "\n",
    "def read_articles(file_path, label):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Check for any empty values in any column\n",
    "    empty_rows = df[df.isnull().any(axis=1)]\n",
    "\n",
    "    # Display rows with empty values\n",
    "\n",
    "    if len(empty_rows)>0:\n",
    "     print(empty_rows)\n",
    "\n",
    "    # Drop rows with missing values in any column\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    labeled_articles = []\n",
    "    for index, row in df.iterrows():\n",
    "        title, text, subject, date = row[['title', 'text', 'subject', 'date']]\n",
    "        article = (title, text, subject, date, label)\n",
    "        labeled_articles.append(article)\n",
    "    return labeled_articles\n",
    "\n",
    "labeled_fake_articles = read_articles(fake_file_path, fake_label)\n",
    "labeled_real_articles = read_articles(real_file_path, real_label)\n",
    "\n",
    "# labeled_fake_articles = []\n",
    "# with open(fake_file_path, 'r', encoding='utf-8') as file:\n",
    "#     csv_file = csv.reader(file)\n",
    "#     headers = next(csv_file)\n",
    "#     for row in csv_file:\n",
    "\n",
    "#         title = row[0]\n",
    "#         text = row[1]\n",
    "#         subject = row[2]\n",
    "#         date = row[3]\n",
    "#         label = fake_label # Assign labels to each article (0 for fake)\n",
    "\n",
    "#         article = (title, text, subject, date, label)\n",
    "\n",
    "#         labeled_fake_articles.append(article)\n",
    "\n",
    "\n",
    "# labeled_real_articles = []\n",
    "# with open(real_file_path, 'r', encoding='utf-8') as file:\n",
    "#     csv_file = csv.reader(file)\n",
    "#     headers = next(csv_file)\n",
    "#     for row in csv_file:\n",
    "\n",
    "#         title = row[0]\n",
    "#         text = row[1]\n",
    "#         subject = row[2]\n",
    "#         date = row[3]\n",
    "#         label = real_label # Assign labels to each article (1 for real)\n",
    "\n",
    "#         article = (title, text, subject, date, label)\n",
    "\n",
    "#         labeled_real_articles.append(article)\n",
    "\n",
    "combined_articles = labeled_real_articles + labeled_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6d64f-cc4b-4c1b-944f-487e762765fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(combined_articles)\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "for i in range(5):\n",
    "    print(combined_articles[i])\n",
    "\n",
    "# Check the distribution of labels\n",
    "fake_count = sum(1 for article in combined_articles if article[4] == fake_label)\n",
    "real_count = sum(1 for article in combined_articles if article[4] == real_label)\n",
    "print(\"Fake Articles:\", fake_count)\n",
    "print(\"Real Articles:\", real_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985841b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Define a list of known abbreviations\n",
    "    abbreviations = [\"U.S.\", \"Dr.\", \"etc.\", \"e.g.\", \"i.e.\"]\n",
    "    \n",
    "   # Separate words that are joined together (e.g., leftNews)\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "       # Check if the word is an abbreviation\n",
    "        found_abbreviation = False\n",
    "        for abbr in abbreviations:\n",
    "            if abbr in word:\n",
    "                # Remove punctuation from the abbreviation\n",
    "                abbr_without_punctuation = ''.join(char for char in abbr if char.isalnum())\n",
    "\n",
    "                # Tokenize the text\n",
    "                tokens.append(abbr_without_punctuation)\n",
    "\n",
    "                found_abbreviation = True\n",
    "                break\n",
    "        \n",
    "        if not found_abbreviation:\n",
    "            # Pattern to handle abbreviations and words with punctuation\n",
    "            tokens.extend(re.findall(r'[A-Z]{2,}(?:\\.[A-Z]\\.)?(?:[,.!?]|$)|[A-Z]?[a-z]+|[A-Z]+|[a-z]+(?=[A-Z])', word))\n",
    "      \n",
    "    # Remove stopwords\n",
    "    stopwords = [\"the\", \"and\", \"is\", \"it\", \"in\", \"to\", \"of\", \"an\", \"a\"]\n",
    "    tokens_without_stopwords = [word for word in tokens if word not in stopwords]\n",
    "   \n",
    "    # Join tokens back into a string and convert text to lowercase\n",
    "    preprocessed_text = ' '.join(tokens_without_stopwords).lower()\n",
    "\n",
    "    text_without_punctuation = re.sub(r'[^\\w\\s]', '', preprocessed_text)\n",
    "\n",
    "    return text_without_punctuation\n",
    "\n",
    "\n",
    "# Preprocess the title, text and subject data in combined_articles\n",
    "preprocessed_articles = []\n",
    "for article in combined_articles:\n",
    "    title, text, subject, date, label = article\n",
    "    preprocessed_title = preprocess(title)\n",
    "    preprocessed_text = preprocess(text)\n",
    "    preprocessed_subject = preprocess(subject)\n",
    "    preprocessed_article = (preprocessed_title, preprocessed_text, preprocessed_subject, date, label)\n",
    "    preprocessed_articles.append(preprocessed_article)\n",
    "\n",
    "# Example of preprocessed article\n",
    "print(\"Original Title:\", combined_articles[0][0])\n",
    "print(\"Original Text:\", combined_articles[0][1])\n",
    "print(\"Original Subject:\", combined_articles[0][2])\n",
    "print(\"Preprocessed Title:\", preprocessed_articles[0][0])\n",
    "print(\"Preprocessed Text:\", preprocessed_articles[0][1])\n",
    "print(\"Preprocessed Subject:\", preprocessed_articles[0][2])\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# text = r'''This is an example\"...(sentence of left-news and leftNews, worldnews, U.S., and Dr., 3, 800, 2019, 21questions with punctuation, numbers and abbreviations like e.g. and i.e. with punctuation and stopwords!'''\n",
    "# preprocessed_text = preprocess(text)\n",
    "# print(\"Original Text:\", text)\n",
    "# print(\"Preprocessed Text:\", preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file path\n",
    "preprocessed_csv_file = 'preprocessed_articles.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(preprocessed_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['preprocessed_title', 'preprocessed_text', 'preprocessed_subject', 'date', 'label'])\n",
    "    \n",
    "    # Write each preprocessed article to the CSV file\n",
    "    for article in preprocessed_articles:\n",
    "        csv_writer.writerow(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Read the preprocessed CSV file\n",
    "df = pd.read_csv(preprocessed_csv_file)\n",
    "\n",
    "# # Check for any empty values in any column\n",
    "# empty_rows = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# # Display rows with empty values\n",
    "# if len(empty_rows)>0:\n",
    "#     print(empty_rows)\n",
    "\n",
    "# Drop rows with missing values in any column\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Example function to analyze sentiment\n",
    "def analyse_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    # Get polarity score (-1 to +1). < 0 indicates negative sentiment, > 0 indicates positive sentiment\n",
    "    polarity = blob.sentiment.polarity\n",
    "    # Classify sentiment based on polarity score\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment analysis to each article's text and add sentiment features\n",
    "df['sentiment'] = df['preprocessed_text'].apply(analyse_sentiment)\n",
    "\n",
    "# Now 'df' contains a new column 'sentiment' indicating the sentiment (positive, negative, or neutral) of each article\n",
    "\n",
    "# Columns to include as features\n",
    "text = df['preprocessed_text'] \n",
    "title = df['preprocessed_title']  \n",
    "subject = df['preprocessed_subject'] \n",
    "label = df['label'] \n",
    "sentiment = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d54be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorize text data using TF-IDF with n-gram range\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=40000)  # Adjust max_features as needed, this includes unigrams and bigrams\n",
    "\n",
    "# Vectorize each text feature separately\n",
    "x_text = vectorizer.fit_transform(text)\n",
    "x_title = vectorizer.fit_transform(title)\n",
    "x_subject = vectorizer.fit_transform(subject)\n",
    "x_sentiment = vectorizer.fit_transform(sentiment)\n",
    "\n",
    "# Concatenate the resulting matrices\n",
    "#X = hstack([x_title, x_text, x_subject, x_sentiment])\n",
    "X = x_title\n",
    "y = label  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, x_train, x_test contain the preprocessed and vectorized text data,\n",
    "# and y_train, y_test contain the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # Adjust the number of estimators as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf_model\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(x_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Validation Classification Report:\\n\", val_report)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\\n\", test_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
